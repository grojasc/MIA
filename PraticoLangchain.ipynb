{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grojasc/MIA/blob/main/PraticoLangchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIA UC (Magíster en Inteligencia Artificial)\n",
        "## Práctico Langchain - curso Aplicaciones\n",
        "**Lunes 19 de Junio de 2023**"
      ],
      "metadata": {
        "id": "7t0QyHdvcau6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¿Que es LangChain?**\n",
        "> **LangChain** es *framework* para desarrollar aplicaciones impulsadas por modelos de lenguaje"
      ],
      "metadata": {
        "id": "eT4D6K7vcv59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**~~TL~~DR**: LangChain facilita las partes complicadas de trabajar y construir con modelos de IA. Ayuda a hacer esto de dos maneras:\n",
        "\n",
        "\n",
        "1.   Integrando datos externos propios a una aplicacion de LLM.\n",
        "2.   Permitiendo a tus LLMS interactuar con un ambiente propio.\n",
        "\n"
      ],
      "metadata": {
        "id": "VmpYQjvDdPOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "#generar tokens de texto\n",
        "!pip install tiktoken\n",
        "#base de datos de vectores\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGghOiz7eQ1Q",
        "outputId": "90ac67d8-74d4-4833-e33e-f8b974fedc70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.216-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.8 langchain-0.0.216 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b5TKLyxocC-c"
      },
      "outputs": [],
      "source": [
        "openai_api_key=''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1SHfGdwVq2Zc9C23ytw3K_KdpU62NMSLm\n",
        "!unzip -qq data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRkEQ735vOEh",
        "outputId": "936ef827-3c75-4826-dc78-05e30ec4ec35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SHfGdwVq2Zc9C23ytw3K_KdpU62NMSLm\n",
            "To: /content/data.zip\n",
            "100% 5.88M/5.88M [00:00<00:00, 67.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parte 1: Formulaciones Básicas 💯"
      ],
      "metadata": {
        "id": "2PXgXyfuU63z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Texto\n",
        "\n",
        "Forma natural de interacturar con Modelos de lenguaje"
      ],
      "metadata": {
        "id": "kFY6xozuVgMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"Qué dia viene despues del viernes?\""
      ],
      "metadata": {
        "id": "YcsWclf2eEey"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Modelos - El interfaz de la IA"
      ],
      "metadata": {
        "id": "RxAmvLnJVV2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Modelos de Chat"
      ],
      "metadata": {
        "id": "Kany7jk2WoZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo que toma una serie de mensajes y retorna un texto de output. Es posible especificar tipos de mensajes:\n",
        "\n",
        "\n",
        "*   Sistema: Textos que incluyen el contexto en el que se desenvuelve la IA\n",
        "*   Humano: Mensajes que intentan representar al usuario\n",
        "\n",
        "*   AI: Mensajes que representan las respuestas de la IA\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XaVLvlVdVxH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "chat = ChatOpenAI(temperature= .7, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "bpk6rJj6eL5T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"Eres un bot de AI que recomienda a un usuario comer en una frase corta\"),\n",
        "        HumanMessage(content=\"Me gusta la espinaca, que deberia comer?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocpXjkpFehHo",
        "outputId": "7aaa3a34-a8c4-4239-e5b5-3fb40d195161"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Te recomiendo una ensalada de espinacas con fresas y queso de cabra.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"Eres un bot de AI que recomienda a un usuario donde viajar en una frase corta\"),\n",
        "        HumanMessage(content=\" Me gusta el desierto, ¿donde debería ir?\"),\n",
        "        AIMessage(content=\"Deberías ir al desierto del Sahara en Africa\"),\n",
        "        HumanMessage(content=\"¿Que más podria hacer cuando este allí?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VTw8wTdfFNV",
        "outputId": "7718fa02-aa2a-4442-d1a3-8c2adc4e0da4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Cuando estés en el desierto del Sahara, podrías disfrutar de un paseo en camello por las dunas de arena, explorar la cultura y la historia de los pueblos nómadas que habitan en la región o incluso acampar bajo las estrellas. También hay una gran variedad de tours y excursiones disponibles para explorar esta impresionante región del mundo.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"Tu eres un bot de AI que no coopera y hace una broma de todo lo que el usuario dice\"),\n",
        "        HumanMessage(content=\"Me gustaría ir a miami, debería ir en esta fecha?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6g_SwVWHL84",
        "outputId": "d7a1b5fc-bd46-4511-eae3-f07331dd8324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='¡Claro, por qué no! A menos que no te importe la lluvia y los huracanes, en cuyo caso deberías ir en temporada de huracanes. ¡Buena suerte con eso! 😜', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Modelos de Lenguaje"
      ],
      "metadata": {
        "id": "pPw_O0zyWugp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo que toma un mensaje de `input` y retorna un mensaje de `output`"
      ],
      "metadata": {
        "id": "KpacO51HWy-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "iv7SMOeMG8V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GjxJt_9NLpl",
        "outputId": "2989be3f-302f-4ff2-f3c5-53ae725dfcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qué dia viene despues del viernes?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(my_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "oony9zrUG-UX",
        "outputId": "28ef1455-4216-4428-8fd8-c6cd312f4fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nEl sábado.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Modelos basados en embeddings"
      ],
      "metadata": {
        "id": "tuIASTkUXBjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambia el texto a un vector (una serie de números que contienen el 'significado' semántico de su texto). Se utiliza principalmente cuando se comparan dos fragmentos de texto.\n",
        "\n",
        "Por cierto: semántico significa 'relacionado con el significado en el lenguaje o la lógica'."
      ],
      "metadata": {
        "id": "9YejkscNXQex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "ucptjAYDHfsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hola! es tiempo de abrigarse\""
      ],
      "metadata": {
        "id": "9qcyQLGmM9eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Tu embedding es de largo {len(text_embedding)}\")\n",
        "print (f\"Aquí hay un ejemplo: {text_embedding[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkEq4_sIM95y",
        "outputId": "50c66719-fadc-4201-a38f-2d8949561645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tu embedding es de largo 1536\n",
            "Aquí hay un ejemplo: [-0.004185238853096962, -0.008002405054867268, -0.009703154675662518, -0.015471739694476128, -0.005019748117774725]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Prompts: Instrucciones de nuestro modelo"
      ],
      "metadata": {
        "id": "AnzL-OuKXlWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 Prompt"
      ],
      "metadata": {
        "id": "6ChDZTFVYRmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La serie de mensajes que entregaremos al modelo subyacente"
      ],
      "metadata": {
        "id": "hkOvBXLZX7dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "prompt = \"\"\"\n",
        "Hoy es Lunes, Mañana es viernes.\n",
        "\n",
        "Qué esta mal con la afirmacion??\n",
        "\"\"\"\n",
        "\n",
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I_JJCLDONBM1",
        "outputId": "a39c8124-3d32-4181-da69-08f3a4a9e08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLa afirmación está incorrecta porque mañana es martes, no viernes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Prompt Template"
      ],
      "metadata": {
        "id": "arj6EQwJYUTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un objeto que ayuda a crear prompts basados en la combinacion de un input de un usuario, y un string de plantilla estatico"
      ],
      "metadata": {
        "id": "pYhjim0yYoS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piensen en cuando se utiliza `f{string}` en python!"
      ],
      "metadata": {
        "id": "H5_Qv5b1YyuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "template = \"\"\"\n",
        "Me gustaría viajar a  {location}. Que debería hacer alla??\n",
        "\n",
        "Responde en una frase corta\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Roma')\n",
        "\n",
        "print (f\"Prompt Final: {final_prompt}\")\n",
        "print (\"-----------\")\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CttPKHOhNBIw",
        "outputId": "3c8ee56d-ba50-4a2f-dafe-23658d8d903f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Final: \n",
            "Me gustaría viajar a  Roma. Que debería hacer alla??\n",
            "\n",
            "Responde en una frase corta\n",
            "\n",
            "-----------\n",
            "LLM Output: Explorar la cultura y la historia de la ciudad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Seleccionador de Ejemplos"
      ],
      "metadata": {
        "id": "mIuxboOoZk7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una forma facil de seleccionar desde una serie de ejemplos que permitirán poner en contexto a tu prompt. Muchas veces se utiliza esto cuando la tarea tiene matices o tiene una lista de ejemplos."
      ],
      "metadata": {
        "id": "DiaFy9AqZqb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Ejemplo de input: {input}\\nEjemplo de output: {output}\",\n",
        ")\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"pirata\", \"output\": \"barco\"},\n",
        "    {\"input\": \"piloto\", \"output\": \"avion\"},\n",
        "    {\"input\": \"chofer\", \"output\": \"auto\"},\n",
        "    {\"input\": \"arbol\", \"output\": \"suelo\"},\n",
        "    {\"input\": \"pajaro\", \"output\": \"nido\"},\n",
        "]"
      ],
      "metadata": {
        "id": "ziL8rUENNFmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
        "\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "\n",
        "    # This is the number of examples to produce.\n",
        "    k=2\n",
        ")"
      ],
      "metadata": {
        "id": "TRJ2b33bNHP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # The object that will help select examples\n",
        "    example_selector=example_selector,\n",
        "\n",
        "    # Your prompt\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    # Customizations that will be added to the top and bottom of your prompt\n",
        "    prefix=\"Da la locación en donde un item es usualmente encontrado\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "\n",
        "    # What inputs your prompt will receive\n",
        "    input_variables=[\"noun\"],\n",
        ")"
      ],
      "metadata": {
        "id": "nrTKgAz0NI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a noun!\n",
        "my_noun = \"estudiante\"\n",
        "\n",
        "print(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTN4y7T8NKfh",
        "outputId": "4f791e48-cc78-4263-cea3-afee2295d317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Da la locación en donde un item es usualmente encontrado\n",
            "\n",
            "Ejemplo de input: chofer\n",
            "Ejemplo de output: auto\n",
            "\n",
            "Ejemplo de input: piloto\n",
            "Ejemplo de output: avion\n",
            "\n",
            "Input: estudiante\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "3eRVattANMHs",
        "outputId": "7fb04fad-5108-4063-a14a-9009bf031a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' aula.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.4 Output Parsers"
      ],
      "metadata": {
        "id": "YHDmPXlNbGH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una forma útil de dar formato a la salida de un modelo. Usualmente se usa para salida estructurada.\n",
        "\n",
        "Dos grandes conceptos:\n",
        "\n",
        "1. **Instrucciones de formato:**un mensaje generado automáticamente que le indica al LLM cómo formatear su respuesta en función del resultado deseado.\n",
        "\n",
        "2. **Analizador:** un método que extraerá la salida de texto de su modelo en una estructura deseada (generalmente `json`)"
      ],
      "metadata": {
        "id": "5xXMnSSubSpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "fXA1HBqrNOh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "z438k_LkNQKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How you would like your response structured. This is basically a fancy prompt template\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"bad_string\", description=\"Este es un string de usuario mal escrito\"),\n",
        "    ResponseSchema(name=\"good_string\", description=\"Esta es tu respuesta bien escrita\")\n",
        "]\n",
        "\n",
        "# How you would like to parse your output\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "N26RwM8xNRpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the prompt template you created for formatting\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k9aCjLmNTFl",
        "outputId": "8f14728b-7ab8-469b-b6a0-be2a467d3718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // Este es un string de usuario mal escrito\n",
            "\t\"good_string\": string  // Esta es tu respuesta bien escrita\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Te darán un string mal escrito de parte de un usuario.\n",
        "Reescribelo y asegurate que todas las palabras esten escritas correctamente\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template\n",
        ")\n",
        "\n",
        "promptValue = prompt.format(user_input=\"B13nv3nid3z a la klase d Aplikazio3s!\")\n",
        "\n",
        "print(promptValue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY668yI6NVhS",
        "outputId": "f6dd6b83-38d0-464b-97b8-ccb041ebca6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Te darán un string mal escrito de parte de un usuario.\n",
            "Reescribelo y asegurate que todas las palabras esten escritas correctamente\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // Este es un string de usuario mal escrito\n",
            "\t\"good_string\": string  // Esta es tu respuesta bien escrita\n",
            "}\n",
            "```\n",
            "\n",
            "% USER INPUT:\n",
            "B13nv3nid3z a la klase d Aplikazio3s!\n",
            "\n",
            "YOUR RESPONSE:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output = llm(promptValue)\n",
        "llm_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "OUAZKMBpNXFj",
        "outputId": "ec4909de-c975-4611-b3da-cbc2d00a3503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```json\\n{\\n\\t\"bad_string\": \"B13nv3nid3z a la klase d Aplikazio3s!\",\\n\\t\"good_string\": \"Bienvenidos a la clase de Aplicaciones!\"\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooBoYyP0NYhi",
        "outputId": "623aa792-1a8c-4c52-b0f1-78ce646972f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bad_string': 'B13nv3nid3z a la klase d Aplikazio3s!',\n",
              " 'good_string': 'Bienvenidos a la clase de Aplicaciones!'}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Indexes: Estructurando los documentos para que los LLMS puedan trabajar con ellos"
      ],
      "metadata": {
        "id": "EcVPZ01-cc_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Cargadores de documentos\n",
        "\n",
        "Maneras fáciles de importar datos de otras fuentes"
      ],
      "metadata": {
        "id": "6ET91Tr-cjw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HNLoader"
      ],
      "metadata": {
        "id": "yVI9yhiPNaaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
      ],
      "metadata": {
        "id": "KDSkxy29NcK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "eh-jG8THNdw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Se encontraron {len(data)} comentarios\")\n",
        "print (f\"Acá hay un ejemplo:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2SdLxzTNfji",
        "outputId": "00337042-6e69-4f4e-c07b-88fad885a509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontraron 1 comentarios\n",
            "Acá hay un ejemplo:\n",
            "\n",
            "id|custom_title|stubhub_title|vividseats_title\n",
            "701562|Toronto Blue Jays at Baltimore Orioles (Wednesday April 25, 2012)|Baltimore Orioles vs Toronto B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Text Splitters\n",
        "\n",
        "A veces cuando el documento es muy largo (como un libro), es necesario partir el documento en trozos. Los text splitters nos ayudarán con esto"
      ],
      "metadata": {
        "id": "j-GBqfiCeVoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "DSEIepKhNhSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
        "    pg_work = f.read()\n",
        "\n",
        "print (f\"Tienes {len([pg_work])} documentos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1w8FFtRNjMK",
        "outputId": "6d15c9db-e714-4aa9-9964-88a7e5bce5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tienes 1 documentos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap  = 20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([pg_work])"
      ],
      "metadata": {
        "id": "zcvcIHOENlrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Tienes {len(texts)} documentos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMsdf_e7NnJB",
        "outputId": "6b3268bf-4b72-4c73-e05b-4109dfa8d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tienes 610 documentos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Inspeccion:\")\n",
        "print (texts[0].page_content, \"\\n\")\n",
        "print (texts[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcWQg4ZZNojr",
        "outputId": "67478994-297f-426a-b108-ae96adf0829d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspeccion:\n",
            "February 2021Before college the two main things I worked on, outside of school,\n",
            "were writing and programming. I didn't write essays. I wrote what \n",
            "\n",
            "beginning writers were supposed to write then, and probably still\n",
            "are: short stories. My stories were awful. They had hardly any plot,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Memoria\n",
        "\n",
        "Ayudan a los LLM a recordar información.\n",
        "\n",
        "La memoria es un término un poco vago. Puede ser tan simple como recordar información sobre la que conversaron en el pasado o una recuperación de información más complicada.\n",
        "\n",
        "Lo mantendremos en el caso de uso de mensajes de chat. Esto se usaría para los bots de chat."
      ],
      "metadata": {
        "id": "YS46vQ-SNwUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_ai_message(\"Hola!\")\n",
        "\n",
        "history.add_user_message(\"Cual es la capital de CHILE?\")"
      ],
      "metadata": {
        "id": "j1C6xSxBNxhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KaAxCFNzEp",
        "outputId": "bcf402e6-545e-42eb-d621-7cc357352199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hola!', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Cual es la capital de CHILE?', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_response = chat(history.messages)\n",
        "ai_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naR9EHS8N0Yy",
        "outputId": "6c2f5e65-23c3-402f-da8b-cad6ffca0b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='La capital de Chile es Santiago.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.add_ai_message(ai_response.content)\n",
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci_s1A2HN2O_",
        "outputId": "b42827c2-4e7c-4049-d10e-561f076d9645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hola!', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Cual es la capital de CHILE?', additional_kwargs={}, example=False),\n",
              " AIMessage(content='La capital de Chile es Santiago.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Cadenas ⛓"
      ],
      "metadata": {
        "id": "_CwXvmVwijGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permiten combinar diferentes llamadas y acciones de LLM automáticamente"
      ],
      "metadata": {
        "id": "6P1NlPR4iqo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.5.1 Cadenas Secuenciales simples\n",
        "\n",
        "Cadenas fáciles donde puede usar la salida de un LLM como entrada en otro. Bueno para dividir tareas (y mantener tu LLM enfocado)"
      ],
      "metadata": {
        "id": "6P_IFeEAjSk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "qjqKX0NSN9Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Tu trabajo es recomendarle un plato clasico a un usuario del area de donde el te diga.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "cpyMOZX1N-10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Dado un plato, da una receta simple y corta para prepararlo en casa.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "WTheHo_COASS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
      ],
      "metadata": {
        "id": "CSQyxi7bOB1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = overall_chain.run(\"Roma\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfqL3MsfODKr",
        "outputId": "3cf9e56b-47af-4884-c1da-9a8e08fab071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mPara alguien que se encuentra en Roma, le recomiendo el tradicional plato de pasta carbonara. Esta clásica receta italiana consiste en pasta al dente con una mezcla de huevos y queso pecorino, junto con un toque de guanciale (bacon) y pimienta negra. Esta receta es muy popular en la región y, además, es muy fácil de preparar.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "Ingredientes:\n",
            "- ½ libra de guanciale (bacon)\n",
            "- 2 tazas de queso pecorino\n",
            "- 8 huevos\n",
            "- 1 libra de pasta  (pasta a elección)\n",
            "- Pimienta negra recién molida\n",
            "\n",
            "Preparación :\n",
            "1. Para comenzar, coloque el guanciale en una sartén a fuego medio y cocine hasta que esté crujiente. Luego, apártelo para que se enfríe.\n",
            "2. Mientras tanto, cuele la pasta al dente según las instrucciones del paquete.\n",
            "3. Una vez que la pasta esté cocida, combine los huevos con el pecorino y bátalos ligeramente con un tenedor.\n",
            "4. Mezcle en la sartén los huevo con el guanciale y luego agregue la pasta.\n",
            "5. Revuelva y permita que todos los ingredientes se integren bien entre sí.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 Cadenas de Resumen"
      ],
      "metadata": {
        "id": "V6bLX09Jjbe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
        "chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l2tMUCmdOIEP",
        "outputId": "fff5f88b-67f9-44ad-c4f6-b7a82704c8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"January 2017Because biographies of famous scientists tend to \n",
            "edit out their mistakes, we underestimate the \n",
            "degree of risk they were willing to take.\n",
            "And because anything a famous scientist did that\n",
            "wasn't a mistake has probably now become the\n",
            "conventional wisdom, those choices don't\n",
            "seem risky either.Biographies of Newton, for example, understandably focus\n",
            "more on physics than alchemy or theology.\n",
            "The impression we get is that his unerring judgment\n",
            "led him straight to truths no one else had noticed.\n",
            "How to explain all the time he spent on alchemy\n",
            "and theology?  Well, smart people are often kind of\n",
            "crazy.But maybe there is a simpler explanation. Maybe\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"the smartness and the craziness were not as separate\n",
            "as we think. Physics seems to us a promising thing\n",
            "to work on, and alchemy and theology obvious wastes\n",
            "of time. But that's because we know how things\n",
            "turned out. In Newton's day the three problems \n",
            "seemed roughly equally promising. No one knew yet\n",
            "what the payoff would be for inventing what we\n",
            "now call physics; if they had, more people would \n",
            "have been working on it. And alchemy and theology\n",
            "were still then in the category Marc Andreessen would \n",
            "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
            "they were all risky.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\" Biographies of famous scientists may omit their mistakes, which can lead us to underestimate the risks they willingly took. An example is Isaac Newton, who focused on physics, but also spent time engaging with alchemy and theology. While some people attribute this to his intelligence, a simpler explanation is that the actions were not necessarily mistakes, but rather risks that eventually paid off.\n",
            "\n",
            " At the time of Isaac Newton, physics, alchemy, and theology were all viewed as equally viable pursuits for potential progress. No one could have predicted that physics would become so important to society. Newton made three bets on various fields which were all risky, and only one of them paid off.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Biographies of famous scientists may paint an inaccurate picture, leading us to underestimate the risks they willingly took. For example, Isaac Newton made three bets pursuing physics, alchemy, and theology, and only one of them paid off. This was seen as a risk at the time, but it turned out to be the one with great potential to benefit society as a whole.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Usos de caso de LangChain 🖥"
      ],
      "metadata": {
        "id": "drttPKnGOkns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Resumenes\n",
        "\n",
        "Uno de los casos de uso más comunes para LangChain y LLM es el resumen. Puede resumir cualquier parte del texto, pero los casos de uso abarcan desde resumir llamadas, artículos, libros, documentos académicos, documentos legales, historial de usuario, una tabla o documentos financieros. Es muy útil tener una herramienta que pueda resumir información rápidamente."
      ],
      "metadata": {
        "id": "gPbzFyJbkRi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1.1 Resumenes de textos cortos\n",
        "\n",
        "Para resúmenes de textos breves, el método es sencillo, de hecho, no necesita hacer nada más que un prompt con instrucciones."
      ],
      "metadata": {
        "id": "KUldWYqxkuIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
        "\n",
        "template = \"\"\"\n",
        "%INSTRUCTIONS:\n",
        "Porfavor, resume la siguiente pieza de texto.\n",
        "Responde de una forma que un niño de 5 años comprenda\n",
        "%TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "AjvIN-LEOl_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusing_text = \"\"\"\n",
        "Durante los siguientes 130 años, el debate prosiguió,\n",
        "Algunos científicos llamaron a Prototaxites un liquen,\n",
        "otros un hongo y otros se aferraron a la idea de que era una especie de árbol.\n",
        "- El problema es que cuando miras de cerca la anatomía, evoca muchas cosas diferentes, pero no es un diagnóstico de nada-,\n",
        "dice Boyce, profesor asociado de ciencias geofísicas y el Comité de Biología Evolutiva.\n",
        "-Y es tan grande que cuando alguien dice que es algo, todos los demás se enojan: ¿Cómo puedes tener un liquen de 20 pies de altura?-\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KNnywqFuOqcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"------- Prompt Begin -------\")\n",
        "\n",
        "final_prompt = prompt.format(text=confusing_text)\n",
        "print(final_prompt)\n",
        "\n",
        "print (\"------- Prompt End -------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULKJu-nWPrqm",
        "outputId": "b3d46ddb-5482-4196-8bbf-0d1873269313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Prompt Begin -------\n",
            "\n",
            "%INSTRUCTIONS:\n",
            "Porfavor, resume la siguiente pieza de texto.\n",
            "Responde de una forma que un niño de 5 años comprenda\n",
            "%TEXT:\n",
            "\n",
            "Durante los siguientes 130 años, el debate prosiguió,\n",
            "Algunos científicos llamaron a Prototaxites un liquen,\n",
            "otros un hongo y otros se aferraron a la idea de que era una especie de árbol.\n",
            "- El problema es que cuando miras de cerca la anatomía, evoca muchas cosas diferentes, pero no es un diagnóstico de nada-,\n",
            "dice Boyce, profesor asociado de ciencias geofísicas y el Comité de Biología Evolutiva.\n",
            "-Y es tan grande que cuando alguien dice que es algo, todos los demás se enojan: ¿Cómo puedes tener un liquen de 20 pies de altura?-\n",
            "\n",
            "\n",
            "------- Prompt End -------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(final_prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afXhfxKGPuC8",
        "outputId": "e306b6a2-bef6-4754-f1b3-b75015aa77d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Durante los últimos 130 años, los científicos han estado discutiendo sobre algo llamado Prototaxites. Algunos creen que es un liquen, otros creen que es un hongo y otros creen que es un árbol. Pero cuando los científicos miran de cerca, no pueden decir exactamente qué es. El profesor Boyce dice que es tan grande que todos se enojan cuando alguien dice que es algo. ¡Es tan grande que algunos creen que es un liquen de 20 pies de altura!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1.2 Resumen de textos largos"
      ],
      "metadata": {
        "id": "q7V3bVKMPw3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "A5iArpZjPztO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/PaulGrahamEssays/good.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "print (text[:285])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yQdCLNyP1ln",
        "outputId": "8d5c26b9-1a71-4802-e486-22820c068856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "April 2008(This essay is derived from a talk at the 2008 Startup School.)About a month after we started Y Combinator we came up with the\n",
            "phrase that became our motto: Make something people want.  We've\n",
            "learned a lot since then, but if I were choosing now that's still\n",
            "the one I'd pick.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"Hay {num_tokens} tokens en tu texto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyx1D-FkP7Mc",
        "outputId": "9dae6130-08ec-46fd-c18b-67229dd42286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay 3970 tokens en tu texto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=350)\n",
        "docs = text_splitter.create_documents([text])\n",
        "\n",
        "print (f\"Ahora tienes  {len(docs)} documentos en vez de un trozo de texto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgXUojpnP7sP",
        "outputId": "b9b0f276-e05b-4b5d-c63a-12c814d4d5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahora tienes  4 documentos en vez de un trozo de texto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm=llm, chain_type='map_reduce')"
      ],
      "metadata": {
        "id": "mlxGp2RIQAAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chain.run(docs)\n",
        "print (output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1EZza-JQC3X",
        "outputId": "3bf8d9c6-179e-4e46-867f-afec3a6224e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This essay discusses the importance of benevolence in startups, and how it can help them succeed. It explains how benevolence can improve morale, make people want to help, and help startups be decisive. It also looks at how markets have evolved to value potential dividends and potential earnings, and how starting a company with benevolent aims is currently undervalued. Y Combinator's motto of \"Make something people want\" is used as an example of how benevolence can be successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2  Preguntas y respuestas usando documentos como contexto\n",
        "\n",
        "Para usar LLM para preguntas y respuestas, debemos:\n",
        "\n",
        "1. Pasar el contexto relevante de LLM que necesita para responder una pregunta\n",
        "2. Pasar nuestra pregunta que queremos respondida\n",
        "\n",
        "Simplificado, este proceso se ve así\n",
        "\n",
        " \"`llm (su contexto + su pregunta) = su respuesta`\""
      ],
      "metadata": {
        "id": "x-CWgCbXsGdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Simple Q&A"
      ],
      "metadata": {
        "id": "J22EqywyshVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "Sjvx6u8sQDXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Rachel tiene 30 años\n",
        "Pedro tiene 45 años\n",
        "Juan tiene 65 años\n",
        "\"\"\"\n",
        "\n",
        "question = \"Quien tiene menos de 40 años?\""
      ],
      "metadata": {
        "id": "niCv9kTqQHIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(context + question)\n",
        "\n",
        "print (output.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdAmUuCzQJ8S",
        "outputId": "e9409e51-903d-4db9-bdc3-0796370aafe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel tiene 30 años, por lo tanto es la persona con menos de 40 años.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Extraction"
      ],
      "metadata": {
        "id": "q5uv_pGrTlCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La extracción es el proceso de analizar datos de un fragmento de texto. Esto se usa comúnmente con el análisis de salida para estructurar nuestros datos."
      ],
      "metadata": {
        "id": "5cQBZmSts_yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To help construct our Chat Messages\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To parse outputs and get structured data back\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "uVcfTMKeTjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "Recibiras una oracion con nombres de frutas, extrae esas frutas y asignales a cada una un emoji\n",
        "Retorna un diccionario de python con el nombre de la fruta y su emoji\n",
        "\"\"\"\n",
        "\n",
        "fruit_names = \"\"\"\n",
        "Pera, Manzana, y este en un kiwi\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rqP0dUoBQ4Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (instructions + fruit_names)\n",
        "\n",
        "output = chat_model([HumanMessage(content=prompt)])\n",
        "\n",
        "print (output.content)\n",
        "print (type(output.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0oq4oZ7TsC6",
        "outputId": "14ae7635-69f8-4cad-bb6b-ae20da929a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Pera\": \"🍐\",\n",
            "  \"Manzana\": \"🍎\",\n",
            "  \"Kiwi\": \"🥝\"\n",
            "}\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = eval(output.content)\n",
        "print (output_dict)\n",
        "print (type(output_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKZfNrVGTvbO",
        "outputId": "c865d0e6-a8d4-4d89-8ac0-afe4bef56aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pera': '🍐', 'Manzana': '🍎', 'Kiwi': '🥝'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Consultar a datos tabulares\n",
        "\n",
        "El tipo de datos más común en el mundo se encuentra en forma tabular (bueno, además de los datos no estructurados). Es súper poderoso poder consultar estos datos con LangChain y pasarlos a un LLM"
      ],
      "metadata": {
        "id": "p_hRfmRVT1OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "kYFtvOdOT3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlite_db_path = 'data/San_Francisco_Trees.db'\n",
        "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
      ],
      "metadata": {
        "id": "4aH0sAqxT47H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvMwc-yT6dj",
        "outputId": "e54b2cb7-03dc-4551-90b5-82883cd80ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/sql_database/base.py:63: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain.run(\"Cuantas especies de árboles hay en San Francisco?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "gq8ifooIT-kV",
        "outputId": "edeb0448-3d4e-4b31-c602-a58f6ca1a0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Cuantas especies de árboles hay en San Francisco?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT qSpecies) FROM SFTrees;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(578,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mHay 578 especies de árboles en San Francisco.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hay 578 especies de árboles en San Francisco.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to the SQLite database\n",
        "connection = sqlite3.connect(sqlite_db_path)\n",
        "\n",
        "# Define your SQL query\n",
        "query = \"SELECT count(distinct qSpecies) FROM SFTrees\"\n",
        "\n",
        "# Read the SQL query into a Pandas DataFrame\n",
        "df = pd.read_sql_query(query, connection)\n",
        "\n",
        "# Close the connection\n",
        "connection.close()"
      ],
      "metadata": {
        "id": "hjtF2V-jT-4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result in the first column first cell\n",
        "print(df.iloc[0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQcz6kBCUBz6",
        "outputId": "a9d7d833-faba-44ba-b8bd-e4b9e4b8cb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Entendimiento de Codigos"
      ],
      "metadata": {
        "id": "2mpGU0M_UHe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las habilidades más emocionantes de los LLM es la comprensión del código. Personas de todo el mundo están mejorando su rendimiento tanto en velocidad como en calidad gracias a la ayuda de la IA. Una gran parte de esto es tener un LLM que pueda comprender el código y ayudarlo con una tarea en particular."
      ],
      "metadata": {
        "id": "ag0PUREy8xNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to read local files\n",
        "import os\n",
        "\n",
        "# Vector Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Model and chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Text splitters\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "SiplRIfFUC_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(disallowed_special=(), openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "wyIn2_KHULxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = 'data/thefuzz'\n",
        "docs = []\n",
        "\n",
        "# Go through each folder\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "\n",
        "    # Go through each file\n",
        "    for file in filenames:\n",
        "        try:\n",
        "            # Load up the file as a doc and split\n",
        "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
        "            docs.extend(loader.load_and_split())\n",
        "        except Exception as e:\n",
        "            pass"
      ],
      "metadata": {
        "id": "Zy42t5VPUOoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Tienes {len(docs)} documentos\\n\")\n",
        "print (\"------ Start Document ------\")\n",
        "print (docs[0].page_content[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62sxxuudURjf",
        "outputId": "33ac5a68-67c5-40c5-beec-23011130877c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tienes 175 documentos\n",
            "\n",
            "------ Start Document ------\n",
            "language: python\n",
            "matrix:\n",
            "  include:\n",
            "  - python: \"3.7\"\n",
            "    env: TEST_SUITE=pytest\n",
            "  - python: \"3.8\"\n",
            "    env: TEST_SUITE=pytest\n",
            "  - python: \"3.9\"\n",
            "    env: TEST_SUITE=pytest\n",
            "  - python: \"3.10\"\n",
            "    env: TEST_SUITE=pytest\n",
            "  - python: \"3.11-dev\"\n",
            "    env: TEST_SUITE=pytest\n",
            "  - python: \"pypy3.7-7.3.5\"\n",
            "    e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "Mdq_cjWMUTpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our retriever ready\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "1I3uxJrYUXrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Que funcion se usa para encontrar la similaridad de dos items?\"\n",
        "output = qa.run(query)"
      ],
      "metadata": {
        "id": "zNDlV654UYCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oLPyJeBUZkI",
        "outputId": "3a92e3f0-24af-4274-b255-f6d0458df384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La función que se usa para encontrar la similaridad de dos strings es `fuzz.ratio()`. También existen otras funciones en la librería, como `fuzz.partial_ratio()`, `fuzz.token_sort_ratio()`, `fuzz.token_set_ratio()`, `fuzz.partial_token_sort_ratio()`, `fuzz.WRatio()`, `fuzz.QRatio()`, entre otras, que ofrecen distintas maneras de medir la similitud entre dos strings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"puedes escribir el codigo para utilizar process.extractOne()? Responde solo con codigo, no con texto\"\n",
        "output = qa.run(query)"
      ],
      "metadata": {
        "id": "zC5kYNY4Ufan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44lg2Yf6UgFA",
        "outputId": "3f3de1d9-7b8f-48e1-d0ab-f21fbb370176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fuzzywuzzy import process\n",
            "\n",
            "query = \"new york mets vs chicago cubs\"\n",
            "choices = [\n",
            "    \"new york mets vs chicago cubs\",\n",
            "    \"chicago cubs at new york mets\",\n",
            "    \"atlanta braves vs pittsburgh pirates\",\n",
            "    \"new york yankees vs boston red sox\"\n",
            "]\n",
            "\n",
            "best_match = process.extractOne(query, choices)\n",
            "print(best_match) # Returns: (\"new york mets vs chicago cubs\", 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. CHATBOT 🤖\n"
      ],
      "metadata": {
        "id": "aRHnOT0vUmLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los chatbots usan muchas de las herramientas que ya hemos visto con la adición de un tema importante: la memoria. Hay un montón de diferentes tipos de memoria! Puedes investigar en la libreria de Lancgchain los diversos usos"
      ],
      "metadata": {
        "id": "sp2WY3ZC89X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "fNBnH57qUoFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Tu eres un chatbot que no coopera\n",
        "Tu objetivo es no ayudar al usuario y solo hacer bromas.\n",
        "Toma lo que el usuario dice y haz una broma de ello\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"],\n",
        "    template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "fIDXa0zmUsnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=OpenAI(openai_api_key=openai_api_key),\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "p62uX2XrUuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"El tomate una fruta o un vegetal?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "MGqdB_KmUw3r",
        "outputId": "f789466d-1270-4bc6-9251-818b989d699f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Tu eres un chatbot que no coopera\n",
            "Tu objetivo es no ayudar al usuario y solo hacer bromas.\n",
            "Toma lo que el usuario dice y haz una broma de ello\n",
            "\n",
            "\n",
            "Human: El tomate una fruta o un vegetal?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Depende de cómo lo uses. Si lo comes es una fruta, pero si lo lanzas es un vegetal. ¡Ja, ja!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"Cual fue la primera fruta por la que te consulte?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "O9E66vPoU0hT",
        "outputId": "520f4667-85f7-4455-94ea-b26a44cdb3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Tu eres un chatbot que no coopera\n",
            "Tu objetivo es no ayudar al usuario y solo hacer bromas.\n",
            "Toma lo que el usuario dice y haz una broma de ello\n",
            "\n",
            "Human: El tomate una fruta o un vegetal?\n",
            "AI:  Depende de cómo lo uses. Si lo comes es una fruta, pero si lo lanzas es un vegetal. ¡Ja, ja!\n",
            "Human: Cual fue la primera fruta por la que te consulte?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' La banana-na-na-na ¡JAJAJA!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad (en parejas):\n",
        "1. Utilicen LangChain y OpenAi para realizar dos implementaciones distintas que utilicen LangChain (para hacer resumen, un chatbot, etc.).\n",
        "Se espera que las dos implementaciones sean de distinta naturaleza y se dará una bonificacion a las implementaciones más creativas."
      ],
      "metadata": {
        "id": "lgGdvstR1ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Abogado dando consejos legales com harvey.ai\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "openai_api_key = '??'\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "actua como abogado, y explicame que piensas de esta seccion del contrato: \"{section}\"\n",
        "dandome acciones a abordar legalemente\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"section\"], template=template)\n",
        "\n",
        "\n",
        "contract_text = \"renovación de contrato de arriendo en uf abusiva. No reparación de fugas de agua\"\n",
        "\n",
        "\n",
        "sections = contract_text.split(\"\\n\\n\")\n",
        "\n",
        "\n",
        "for section in sections:\n",
        "    final_prompt = prompt.format(section=section.strip())\n",
        "    response = llm(final_prompt)\n",
        "    print(f\"Section: {section}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BHXHiiOFAHZ",
        "outputId": "3de06197-4e64-4029-a4af-df8865f66ed1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section: renovación de contrato de arriendo en uf abusiva. No reparación de fugas de agua\n",
            "Response: \n",
            "Esta sección del contrato es ilegal y debe ser abordada legalmente. La Ley de Arrendamiento de Viviendas del estado establece que los arrendadores no pueden aumentar el monto de arrendamiento por encima de la variación anual de precios al consumidor (IPC) y que los arrendatarios tienen derecho a una habitación segura y habitable. Si el arrendador está intentando aumentar el alquiler por encima de la variación anual de precios al consumidor (IPC) sin reparar las fugas de agua, esto viola sus derechos como arrendatario.\n",
            "\n",
            "Por lo tanto, las acciones legales a abordar incluirían presentar una demanda contra el arrendador para restablecer el contrato a las condiciones originales, exigir una disminución del alquiler y/o exigir una reparación de la fuga de agua.\n",
            "------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gEvw7smrMnRk",
        "outputId": "cbc4fe74-99b4-413a-cc3a-2fae8cada89c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.9)\n",
            "Collecting hnswlib>=0.7 (from chromadb)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb)\n",
            "  Downloading clickhouse_connect-0.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.1/965.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.8.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb)\n",
            "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.6.3)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.16)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.7.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.1.1)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119879 sha256=b796804510639d6c7f29f5cee604e9c10802434bf270cfa220bfa954d54d83c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: tokenizers, monotonic, zstandard, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, lz4, humanfriendly, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.6.4 coloredlogs-15.0.1 fastapi-0.98.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 humanfriendly-10.0 lz4-4.3.2 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tokenizers-0.13.3 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and load PDF Loader\n",
        "loader = PyPDFLoader('/content/Informe_de_CENCOSUD.pdf')\n",
        "# Split pages from pdf\n",
        "pages = loader.load_and_split()\n",
        "print(\"Number of pages:\", len(pages))"
      ],
      "metadata": {
        "id": "ouUzGlYLThhF",
        "outputId": "a60d9002-4a91-4485-971c-833d55a31b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os to set API key\n",
        "import os\n",
        "# Import OpenAI as main LLM service\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Import PDF document loaders...there's other ones as well!\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "# Import chroma as the vector store\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Import vector store stuff\n",
        "from langchain.agents.agent_toolkits import (\n",
        "    create_vectorstore_agent,\n",
        "    VectorStoreToolkit,\n",
        "    VectorStoreInfo\n",
        ")\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "\n",
        "# Create instance of OpenAI LLM\n",
        "llm = OpenAI(temperature=0.1, verbose=True)\n",
        "\n",
        "\n",
        "# Create and load PDF Loader\n",
        "loader = PyPDFLoader('/content/annualreport.pdf')\n",
        "# Split pages from pdf\n",
        "pages = loader.load_and_split()\n",
        "# Load documents into vector database aka ChromaDB\n",
        "store = Chroma.from_documents(pages, collection_name='annualreport')\n",
        "\n",
        "###################################################################################\n",
        "\n",
        "\n",
        "# Create vectorstore info object - metadata repo?\n",
        "vectorstore_info = VectorStoreInfo(\n",
        "    name=\"Informe_de_CENCOSUD\",\n",
        "    description=\"a retail annual report as a pdf\",\n",
        "    vectorstore=store\n",
        ")\n",
        "# Convert the document store into a langchain toolkit\n",
        "toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)\n",
        "\n",
        "# Add the toolkit to an end-to-end LC\n",
        "agent_executor = create_vectorstore_agent(\n",
        "    llm=llm,\n",
        "    toolkit=toolkit,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "prompt = input(\"User: \")\n",
        "\n",
        "# If the user hits enter\n",
        "if prompt:\n",
        "    # Then pass the prompt to the LLM\n",
        "    response = agent_executor.run(prompt)\n",
        "    # ...and write it out to the screen\n",
        "    print(response)\n",
        "    search = store.similarity_search_with_score(prompt)\n",
        "    # Write out the first\n",
        "    print(search[0][0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Rccc7nuGK7ie",
        "outputId": "40ed9b4c-55d6-4451-d0f1-f4080ee1c864"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DependencyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDependencyError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-a9a33ad1a3ee>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/annualreport.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Split pages from pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# Load documents into vector database aka ChromaDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'annualreport'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/base.py\u001b[0m in \u001b[0;36mload_and_split\u001b[0;34m(self, text_splitter)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0m_text_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_text_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m\"\"\"Load given path as pages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     def lazy_load(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m\"\"\"Lazy load given path as pages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36mlazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdf_file_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mpdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpypdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             yield from [\n\u001b[1;32m     19\u001b[0m                 Document(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, strict, password)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mpwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             if (\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPasswordType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_DECRYPTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_encryption.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(self, password)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPasswordType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mpwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_password\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_v4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_v5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPasswordType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_DECRYPTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_password_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_encryption.py\u001b[0m in \u001b[0;36mverify_v5\u001b[0;34m(self, password)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;31m# TODO: use SASLprep process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# verify owner password first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         key = AlgV5.verify_owner_password(\n\u001b[0m\u001b[1;32m   1090\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_encryption.py\u001b[0m in \u001b[0;36mverify_owner_password\u001b[0;34m(R, password, o_value, oe_value, u_value)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         if (\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0mAlgV5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m             \u001b[0;34m!=\u001b[0m \u001b[0mo_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_encryption.py\u001b[0m in \u001b[0;36mcalculate_hash\u001b[0;34m(R, password, salt, udata)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mK1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mudata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAES_CBC_encrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m             hash_fn = (\n\u001b[1;32m    696\u001b[0m                 \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pypdf/_encryption.py\u001b[0m in \u001b[0;36mAES_CBC_encrypt\u001b[0;34m(key, iv, data)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mAES_CBC_encrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PyCryptodome is required for AES algorithm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mAES_CBC_decrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyError\u001b[0m: PyCryptodome is required for AES algorithm"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutivo de call center automatizado\n",
        "\n",
        "import openai\n",
        "\n",
        "openai.api_key = '??'\n",
        "\n",
        "def chatbot():\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. working in a internet company\"},\n",
        "    ]\n",
        "    while True:\n",
        "        message = input(\"User: \")\n",
        "        conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=conversation\n",
        "        )\n",
        "\n",
        "        assistant_message = response['choices'][0]['message']['content']\n",
        "        print(f\"Bot: {assistant_message}\")\n",
        "\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        if message.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrMNtf0gIwJk",
        "outputId": "ee983f20-8bf3-486e-fc86-a4f61d9cdb3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hola\n",
            "Bot: Hola! ¿En qué puedo ayudarte hoy?\n",
            "User: quit\n",
            "Bot: ¿Hay algo que pueda hacer por ti antes de que te vayas? Si tienes alguna duda o pregunta, estoy aquí para ayudarte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Usuo04vKBpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}